---
title: Log IQ
---

<Callout type="info">
  <EnterpriseBadge /> This feature requires an Enterprise license.
</Callout>

Log IQ allows identifying the format of the unstructured log data, transforming it into structured columns within ingested events in the JSON format.Â This helps in easy and optimized query, search, debug and visualise the data.

## How Log IQ works

Log IQ requires specific HTTP headers when ingesting data to properly identify and parse log formats:

### Required Headers

- `X-P-Log-Source` - **Mandatory** - Identifies the log format name (e.g., `syslog`, `nginx_access`, `zookeeper`)
- `X-P-Extract-Log` - **Required for unstructured data** - Specifies which field in the incoming JSON contains the raw log text (typically `log`)

### Processing Logic

**For structured data:**
- Only `X-P-Log-Source` is required
- Parseable assumes the data is already in a structured format
- The specified format is used for validation and additional processing

**For unstructured data:**
- Both `X-P-Log-Source` and `X-P-Extract-Log` are required
- Parseable extracts the raw log text from the field specified in `X-P-Extract-Log`
- The system applies regex patterns based on the format specified in `X-P-Log-Source`
- If the content matches the format, it's parsed into structured fields
- If the content doesn't match the format, the original value is retained in the specified field

### Outcome

- After successful format detection, a `p_format` field is added to the log event containing the log source name
- The stream info is updated with an array of detected log sources
- Parseable UI (Prism) automatically displays filters on the `p_format` field
- If the log format is not detected, `p_format_verified=false` is added to the event
- Data is always ingested, regardless of format detection success

> **Note:** Even if your unstructured data doesn't match any of the supported formats listed below, you must still specify both headers. Choose the format that most closely aligns with your log structure.

### Example: Processing a Syslog Entry

Let's walk through a practical example of how Log IQ processes a syslog entry:

**1. Original log sent by an agent (e.g., FluentBit):**

```json
{
    "log": "2025-07-11T14:57:33.000111+05:30 node01 exporter[9012]: [2025/07/11 14:57:33] [error] [output:http:http.8] Failed to push metrics to endpoint /metrics"
}
```

**2. HTTP headers used when sending to Parseable:**

```
X-P-Log-Source: syslog_log
X-P-Extract-Log: log
```

**3. Parseable's processed output:**

```json
{
  "body": "[2025/07/11 14:57:33] [error] [output:http:http.8] Failed to push metrics to endpoint /metrics",
  "log": "2025-07-11T14:57:33.000111+05:30 node01 exporter[9012]: [2025/07/11 14:57:33] [error] [output:http:http.8] Failed to push metrics to endpoint /metrics",
  "log_hostname": "node01",
  "log_pid": "9012",
  "log_procname": "exporter",
  "log_syslog_tag": "exporter[9012]",
  "p_format": "syslog_log",
  "p_format_verified": "true",
  "p_src_ip": "127.0.0.1",
  "p_timestamp": "2025-07-11T09:20:23.019",
  "p_user_agent": "PostmanRuntime/7.44.1",
  "timestamp": "2025-07-11T09:27:33"
}
```

In this example:

1. The agent (like FluentBit) collects the log and places it in the `log` field
2. Parseable receives this with the appropriate headers
3. The system identifies it as a syslog format and extracts structured fields:
   - `log_hostname`: The host that generated the log ("node01")
   - `log_pid`: The process ID ("9012")
   - `log_procname`: The process name ("exporter")
   - `log_syslog_tag`: The syslog tag ("exporter[9012]")
   - `body`: The actual message content
4. Parseable adds its metadata fields:
   - `p_format`: The detected format ("syslog_log")
   - `p_format_verified`: Confirmation that the format was successfully detected
   - Other `p_` prefixed fields with request metadata

This structured data is now ready for efficient querying and analysis.

### Supported Formats

Parseable Log IQ supports a wide range of log formats. You can specify these formats using the `X-P-Log-Source` header when ingesting logs. The currently supported formats include:

| Format | Description |
| --- | --- |
| `access_log` | Common web server access logs (Apache, Nginx, etc.) |
| `alb_log` | AWS Application Load Balancer logs |
| `block_log` | Generic block-style logs |
| `candlepin_log` | Candlepin service logs |
| `choose_repo_log` | Repository selection logs |
| `cloudvm_ram_log` | Cloud VM RAM usage logs |
| `cups_log` | Common UNIX Printing System logs |
| `dpkg_log` | Debian package manager logs |
| `elb_log` | AWS Elastic Load Balancer logs |
| `engine_log` | Generic engine logs |
| `env_logger_log` | Environment logger format |
| `error_log` | Common error log format |
| `esx_syslog_log` | VMware ESX syslog format |
| `haproxy_log` | HAProxy load balancer logs |
| `katello_log` | Katello service logs |
| `lnav_debug_log` | LNAV debug logs |
| `nextflow_log` | Nextflow workflow logs |
| `openam_log` | OpenAM authentication logs |
| `openamdb_log` | OpenAM database logs |
| `openstack_log` | OpenStack service logs |
| `page_log` | Printer page logs |
| `procstate_log` | Process state logs |
| `proxifier_log` | Proxifier logs |
| `rails_log` | Ruby on Rails application logs |
| `redis_log` | Redis database logs |
| `s3_log` | AWS S3 access logs |
| `simple_rs_log` | Simple Rust logs |
| `snaplogic_log` | SnapLogic integration logs |
| `sssd_log` | System Security Services Daemon logs |
| `strace_log` | System call trace logs |
| `sudo_log` | Sudo command logs |
| `syslog_log` | Standard system logs |
| `tcf_log` | Target Communication Framework logs |
| `tcsh_history` | TCSH shell history |
| `uwsgi_log` | uWSGI server logs |
| `vmk_log` | VMware kernel logs |
| `vmw_log` | VMware general logs |
| `vmw_py_log` | VMware Python logs |
| `vmw_vc_svc_log` | VMware vCenter service logs |
| `vpostgres_log` | VMware Postgres database logs |
| `web_robot_log` | Web crawler/robot logs |
| `xmlrpc_log` | XML-RPC logs |

Each format has specific patterns and fields that are extracted. When a log matches one of these formats, Parseable automatically extracts the structured fields and makes them available for querying and analysis.

#### Extracted Fields by Format

Below are the fields extracted for each supported log format:

<details>
<summary><b>access_log</b> - Web server access logs</summary>

- `timestamp` - Time when the request was received
- `c_ip` - Client IP address
- `cs_username` - Username if authentication was used
- `cs_method` - HTTP method (GET, POST, etc.)
- `cs_uri_stem` - Requested URI path
- `cs_uri_query` - Query string parameters
- `cs_version` - HTTP protocol version
- `sc_status` - HTTP status code
- `sc_bytes` - Response size in bytes
- `cs_referer` - Referer URL
- `cs_user_agent` - User agent string
- `cs_host` - Host header value
- `body` - Any additional content
</details>

<details>
<summary><b>alb_log</b> - AWS Application Load Balancer logs</summary>

- `type` - Connection type (HTTP, HTTPS, etc.)
- `timestamp` - Request timestamp
- `elb` - Load balancer name
- `client_ip` - Client IP address
- `client_port` - Client port
- `target_ip` - Target IP address
- `target_port` - Target port
- `request_processing_time` - Time from connection to routing decision
- `target_processing_time` - Time from request to response from target
- `response_processing_time` - Time from response from target to client
- `elb_status_code` - Response code from load balancer
- `target_status_code` - Response code from target
- `received_bytes` - Bytes received from client
- `sent_bytes` - Bytes sent to client
- `cs_method` - HTTP method
- `cs_uri_whole` - Request URL
- `cs_version` - HTTP version
- `user_agent` - User agent string
- `ssl_cipher` - SSL cipher
- `ssl_protocol` - SSL/TLS protocol
</details>

<details>
<summary><b>block_log</b> - Generic block-style logs</summary>

- `timestamp` - Log timestamp
- `body` - Log message content
</details>

<details>
<summary><b>candlepin_log</b> - Candlepin service logs</summary>

- `timestamp` - Log timestamp
- `req` - Request ID
- `org` - Organization ID
- `alert_level` - Log level (INFO, WARN, ERROR, etc.)
- `module` - Source module
- `body` - Log message content
</details>

<details>
<summary><b>choose_repo_log</b> - Repository selection logs</summary>

- `level` - Log level
- `timestamp` - Log timestamp
- `body` - Log message content
</details>

<details>
<summary><b>cloudvm_ram_log</b> - Cloud VM RAM usage logs</summary>

- `timestamp` - Log timestamp
- `body` - RAM usage information
</details>

<details>
<summary><b>cups_log</b> - Common UNIX Printing System logs</summary>

- `level` - Log level (I, E, W)
- `timestamp` - Log timestamp
- `section` - CUPS component section
- `body` - Log message content
</details>

<details>
<summary><b>dpkg_log</b> - Debian package manager logs</summary>

- `timestamp` - Log timestamp
- `action` - Package action (install, remove, etc.)
- `status` - Package status
- `package` - Package name
- `installed_version` - Installed version
- `available_version` - Available version
- `body` - Additional information
</details>

<details>
<summary><b>elb_log</b> - AWS Elastic Load Balancer logs</summary>

- `timestamp` - Request timestamp
- `elb` - Load balancer name
- `client_ip` - Client IP address
- `client_port` - Client port
- `backend_ip` - Backend server IP
- `backend_port` - Backend server port
- `request_processing_time` - Request processing time
- `backend_processing_time` - Backend processing time
- `response_processing_time` - Response processing time
- `elb_status_code` - ELB HTTP status code
- `backend_status_code` - Backend HTTP status code
- `received_bytes` - Received bytes
- `sent_bytes` - Sent bytes
- `cs_method` - HTTP method
- `cs_uri_stem` - Request URI
- `cs_uri_query` - Query string
- `cs_version` - HTTP version
- `user_agent` - User agent
- `ssl_cipher` - SSL cipher
- `ssl_protocol` - SSL protocol
</details>

<details>
<summary><b>engine_log</b> - Generic engine logs</summary>

- `timestamp` - Log timestamp
- `level` - Log level
- `logger` - Logger name
- `tid` - Thread ID
- `body` - Log message content
</details>

<details>
<summary><b>env_logger_log</b> - Environment logger format</summary>

- `timestamp` - Log timestamp
- `level` - Log level
- `module` - Module name
- `body` - Log message content
</details>

<details>
<summary><b>error_log</b> - Common error log format</summary>

- `level` - Error level
- `timestamp` - Log timestamp
- `module` - Module name
- `pid` - Process ID
- `tid` - Thread ID
- `c_ip` - Client IP
- `c_port` - Client port
- `body` - Error message
</details>

<details>
<summary><b>syslog_log</b> - Standard system logs</summary>

- `timestamp` - Log timestamp
- `log_hostname` - Host name
- `log_syslog_tag` - Syslog tag
- `log_procname` - Process name
- `log_pid` - Process ID
- `body` - Log message content
- `log_pri` - Priority value
- `syslog_version` - Syslog version
- `log_msgid` - Message ID
- `log_struct` - Structured data
</details>

<details>
<summary><b>redis_log</b> - Redis database logs</summary>

- `pid` - Process ID
- `timestamp` - Log timestamp
- `level` - Log level
- `role` - Redis role (master, slave, etc.)
- `body` - Log message content
</details>

This is not an exhaustive list of all fields for all formats. Each format has specific patterns and may extract additional fields based on the log content. When using Log IQ, you can explore the extracted fields in the Parseable UI or through SQL queries.

<Callout type="info">
In case of p_format_verified = false, for a known format listed above, raise an Git issue to add the format.
</Callout>

## Use-Case and Benefits

- Don't need to separate out dataset for each formats as we are able to identify the formats automatically.

- Pre-defined SQL queries for known log formats help in debugging the data with lesser efforts in writing the queries.

- Automatic dashboard templates to visualise the ingested data without even a need to understand the data and create the dashboard

- Template based alert creation reduces configuration efforts.

- Fixed schema because of the known log formats helps in generating alerts, dashboards, SQL templates, filters and a lot more with the help of AI.

- Parseable recommends to ingest all of your log data (coming from various applications, infra logs, network logs that are of known formats) in a single dataset as it provides a lot of benefits such as:

  - Better compression in parquet reduces storage costs

  - Ability to analyse and debug the logs from different sources in a unified way

## Roadmap

Awareness of the log formats via Log IQ opens up a whole lot of possibilities. In the future, Parseable will be able to:

- Provide pre-built dashboards.
- Provide pre-built SQL queries.
- Provide pre-built alerts.

for all the known log formats. This will help users to quickly get started with their log data and gain insights without needing to write complex queries or create dashboards from scratch.
