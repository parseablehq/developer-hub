---
title: Overview
description: Complete observability for LLM applications, agents, and AI workflows
---

Build reliable AI applications with comprehensive observability for your LLM stack.

## Why LLM Observability?

Large Language Models and AI agents introduce unique observability challenges:

- **Non-deterministic outputs** - Same input can produce different results
- **Complex chains** - Multi-step workflows with multiple LLM calls
- **Cost tracking** - Token usage and API costs add up quickly
- **Latency monitoring** - Response times vary significantly
- **Quality assurance** - Detecting hallucinations and errors

Parseable provides purpose-built observability for LLM applications, helping you understand, debug, and optimize your AI systems.

## Supported Frameworks

Collect traces, logs, and metrics from popular LLM frameworks:

| Framework | Type | Key Metrics |
|-----------|------|-------------|
| [OpenAI](./openai) | API Provider | Tokens, latency, model, cost |
| [Anthropic](./anthropic) | API Provider | Tokens, latency, model, cost |
| [LangChain](./langchain) | Framework | Chain execution, tool calls, retrieval |
| [LlamaIndex](./llamaindex) | Framework | Query engine, indexing, retrieval |
| [AutoGen](./autogen) | Agents | Multi-agent conversations, tool use |
| [CrewAI](./crewai) | Agents | Crew execution, task completion |
| [DSPy](./dspy) | Framework | Compilation, optimization metrics |
| [n8n](./n8n) | Workflow | Workflow execution, node metrics |

## OpenTelemetry Compatible Tools

Parseable natively supports OpenTelemetry traces. Any LLM tool or framework that can export telemetry to an OTLP endpoint can send data directly to Parseable.

This means you're not limited to the integrations listed above. If your LLM tool supports OpenTelemetry output, simply configure it to send traces to Parseable's OTLP endpoint:

```bash
# OTLP HTTP endpoint
export OTEL_EXPORTER_OTLP_ENDPOINT="http://parseable:8000/v1/traces"
```

### Examples of OTel-Compatible LLM Tools

- Traceloop OpenLLMetry - Auto-instrumentation for LLM applications
- Arize Phoenix - LLM observability with OTel export
- Langfuse - Can export to OTLP endpoints
- Helicone - Supports OpenTelemetry output
- Any custom instrumentation using OpenTelemetry SDKs

For detailed setup instructions, see the [OpenTelemetry integration guide](/docs/ingest-data/otel).
